{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBTrACS Valid Training Dates\n",
    "\n",
    "So far, we've seen how to pull GOES imagery and how to overlay IBTrACS data on the imagery. We'll use these same ideas now to obtain dates to fetch training data. Some things we'll need to consider are:\n",
    "\n",
    "* Some bands aren't visible at night, so we don't want to fetch images that aren't useful.\n",
    "    * To address this, we can start by using only infrared bands on the imager that capture clouds at night. If more data is needed, we can come back to this point and identify daytime visible bands.\n",
    "* The geostationary view of the imager has a limited extent, so some IBTrACS data points may not be visible to the imager.\n",
    "    * We can use the projection data of the GOES image filter out the latitudes and longitudes of the IBTrACS data.\n",
    "* GOES16 was launched in November 2016, so we'll only be able to use a subset of the IBTrACS data.\n",
    "    * We just need to make sure we exclude any IBTrACS data outside of the GOES16 imaging window.\n",
    "* IBTrACS data is every 3 hours, while GOES16 is every 5 minutes.\n",
    "    * We can assume that if a storm had a track observation at time $t$ and again at time $t$+3 hours, an image within that time has a storm in it.\n",
    "\n",
    "Each of these points will play a role in how we automatically assess if an image is a positive or negative training sample.\n",
    "\n",
    "Additionally, it seems that the first GOES image, at least that I can access on the AWS storage, wasn't available until 10 April, so we need to filter those dates from the final results.\n",
    "\n",
    "Downloading the data will take some time, so first, I just want to build a list of dates that, according to the IBTrACS data, correspond to a time when a tropical cyclone was present ($y=1$) or was not present ($y=0$). At first, I think the goal should be to get an even number of positive and negative training samples, so I think it might be best to randomly pick a time for a positive example, then ensure that the next selection is a negative example, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "\n",
    "First, let's import the modules we'll need. I've created some functions to do things like read in the IBTrACS and GOES data, which I implemented in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import datetime\n",
    "import numpy as np\n",
    "from pyproj import Proj\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Import functions I've written\n",
    "os.chdir(\"..\")\n",
    "import goes\n",
    "import ibtracs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read IBTrACS Data\n",
    "\n",
    "Now we read in the IBTrACS data from 2017 until now. Let's get rid of anything else before 10 April."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibtracsPath = ibtracs.download_data(basin=\"ALL\",overwrite=False)\n",
    "dfTracks = ibtracs.read_data(ibtracsPath,True,2017,2020)\n",
    "dfTracks = dfTracks[dfTracks['ISO_TIME']>=datetime.datetime(2017,4,10)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GOES16 Image\n",
    "\n",
    "Next, we'll read in the GOES16 image corresponding to 30 days ago. The image itself doesn't matter yet. Since the imager is geostationary, we can just take any image and use it to make sure our IBTrACS data all falls within the GOES16 full disc array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters to download data\n",
    "date = datetime.datetime.now()-datetime.timedelta(days=30)\n",
    "bucketName = 'noaa-goes16'\n",
    "product = 'ABI-L1b-RadF'\n",
    "credPath = \"secrets.csv\"\n",
    "band = 3\n",
    "\n",
    "# Get the GOES data\n",
    "ds = goes.download_data(date,credPath,bucketName,product,band)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Projections\n",
    "\n",
    "In order to get the IBTrACS data relative to the GOES imagery, we need to get the projection of the GOES data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset projection data\n",
    "satHeight = ds.goes_imager_projection.perspective_point_height\n",
    "satLon = ds.goes_imager_projection.longitude_of_projection_origin\n",
    "satSweep = ds.goes_imager_projection.sweep_angle_axis\n",
    "majorMinorAxes = (ds.goes_imager_projection.semi_major_axis,ds.goes_imager_projection.semi_minor_axis)\n",
    "\n",
    "# The projection x and y coordinates equals the scanning angle (in radians) multiplied by the satellite height\n",
    "x = ds.variables['x'][:] * satHeight\n",
    "y = ds.variables['y'][:] * satHeight\n",
    "\n",
    "# Create X and Y meshgrids\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Create a pyproj geostationary map object\n",
    "p = Proj(proj='geos', h=satHeight, lon_0=satLon, sweep=satSweep)\n",
    "\n",
    "# Get latitudes and longitudes\n",
    "lons, lats = p(X, Y, inverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out with Bounding Box\n",
    "\n",
    "A bounding box corresponding to the minimum and maximum latitude and longitudes covers more space than the full-disc, but the only way to really check is to loop through *all* of the IBTrACS data, project it onto the GOES projection, and then see if it's in the image. That will take a lot of time, where this will not, so let's use a bounding box as a first pass here to avoid unnecessary loopling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a simple bounding box based on min/max lat/lons\n",
    "lons = np.where(lons==1e+30,np.nan,lons)\n",
    "lats = np.where(lats==1e+30,np.nan,lats)\n",
    "minLat = np.nanmin(lats)\n",
    "maxLat = np.nanmax(lats)\n",
    "minLon = np.nanmin(lons)\n",
    "maxLon = np.nanmax(lons)\n",
    "\n",
    "# Query IBTraCS data based on bounding box\n",
    "dfTracks = dfTracks[(dfTracks['LAT'] >= minLat) & (dfTracks['LAT'] <= maxLat) & (dfTracks['LON'] >= minLon) & (dfTracks['LON'] <= maxLon)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Additional Off-Disc Samples \n",
    "\n",
    "Now that we've limited the extent a bit, let's drill down and make sure none of the points are off of the full-disc. First, we find the point on the image that corresponds to the latitude/longitude of the storm. Since the `lats` and `lons` arrays have values of `NaN` where the points are off the disc, we can check if that point in the `lat` or `lon` array is missing. We only need to check one array, since it's a meshgrid, se we'll check `lons`. And rather than just checking if that one point is `NaN`, let's check if any point within a window of size `checkSize` is `NaN`. This will avoid any points that are just barely sitting on the edge of the disc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create empty list\n",
    "dropInds = []\n",
    "\n",
    "# Reset indices of dataframe\n",
    "dfTracks = dfTracks.reset_index()\n",
    "\n",
    "for dfInd, row in dfTracks[['LAT','LON']].iterrows():\n",
    "    \n",
    "    # Cast latitude and longitude to float\n",
    "    trackLat = float(row[\"LAT\"])\n",
    "    trackLon = float(row[\"LON\"])\n",
    "\n",
    "    # Convert lon/lat to x/y\n",
    "    trackX,trackY = p(trackLon,trackLat)\n",
    "\n",
    "    # Get the closest point to the IBTrACS data\n",
    "    xInd = np.nanargmin(abs(x-trackX))\n",
    "    yInd = np.nanargmin(abs(y-trackY))\n",
    "\n",
    "    # Check that none of 50 points on any side of the storm are off of the disc\n",
    "    checkSize = 50\n",
    "    offDisc = np.isnan(lons[yInd-checkSize:yInd+checkSize,xInd-checkSize:xInd+checkSize]).any()\n",
    "\n",
    "    # If the points are off the disc, append the dataframe index to drop after looping\n",
    "    if offDisc:\n",
    "        dropInds.append(dfInd)\n",
    "\n",
    "# Drop any indices that fell off the disc\n",
    "dfTracks = dfTracks.drop(dfTracks.index[dropInds])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Output\n",
    "\n",
    "So now, we've handled the issues of the imager extent and the IBTrACS extent, both spatially and temporally. Let's save the output now so we can use it to make training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTracks.to_csv('./data/ibtracs_GOES16.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
